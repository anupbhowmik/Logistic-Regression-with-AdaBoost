{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, option : str='mean'):\n",
    "    \"\"\"Fill missing values in data frame in place\"\"\"\n",
    "    \n",
    "    missing_values = df.isna().sum()\n",
    "    # print(missing_values)\n",
    "\n",
    "    # Check if there are any missing values\n",
    "    if missing_values.any():\n",
    "        print(\"There are missing values in the dataset.\")\n",
    "        # Display the count of missing values for each column\n",
    "        # find the columns with missing values\n",
    "        columns_with_missing_values = df.columns[missing_values > 0]\n",
    "        print(\"Missing values per column:\")\n",
    "        print(missing_values[columns_with_missing_values])\n",
    "\n",
    "        # fix the missing values\n",
    "        # we can use df.replace(to_replace=' ', value=-1) to replace all missing values with -1\n",
    "        for column in columns_with_missing_values:\n",
    "\n",
    "            if option == 'drop':\n",
    "                df.dropna()\n",
    "            elif option == 'mean':\n",
    "                df[column].fillna(df[column].mean(), inplace=True)\n",
    "            elif option == 'median':\n",
    "                df[column].fillna(df[column].median(), inplace=True)\n",
    "            elif option == 'mode':\n",
    "                df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "            elif option == 'linear':\n",
    "                df[column].interpolate(method='linear', limit_direction='forward', inplace=True)\n",
    "            elif option == 'quadratic':\n",
    "                df[column].interpolate(method='quadratic', limit_direction='forward', inplace=True)\n",
    "            elif option == 'cubic':\n",
    "                df[column].interpolate(method='cubic', limit_direction='forward', inplace=True)\n",
    "            elif option == 'spline':\n",
    "                df[column].interpolate(method='spline', order=3, limit_direction='forward', inplace=True)\n",
    "        \n",
    "        print(\"Missing values per column are fixed\\n\")\n",
    "\n",
    "    else:\n",
    "        print(\"There are no missing values in the dataset.\\n\")\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_numeric_data(df, numeric_cols):\n",
    "    \"\"\"Normalize numeric columns in dataframe in place\"\"\"\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "    # print(numeric_cols)\n",
    "    scaler = MinMaxScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_encode(df, categorical_cols):\n",
    "    \"\"\"One hot encode categorical columns in dataframee\"\"\"\n",
    "    one_hot_encoded_data  = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "    categorical_cols = one_hot_encoded_data.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    label_encode(one_hot_encoded_data, categorical_cols)\n",
    "\n",
    "    return one_hot_encoded_data \n",
    "\n",
    "    \n",
    "def label_encode(df, categorical_cols):\n",
    "    \"\"\"Label encode categorical columns in dataframe in place\"\"\"\n",
    "\n",
    "    # print(categorical_cols)\n",
    "    \n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        label_encoders[col] = LabelEncoder()\n",
    "        df[col] = label_encoders[col].fit_transform(df[col])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_score_feature_df(X, y, k=15):\n",
    "    \"\"\"Get the top k features with the highest score\n",
    "        Return a new dataframe with the top k features\n",
    "    \"\"\"\n",
    "    # todo: need to implement this function\n",
    "    from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "    print (\"k:\",k)\n",
    "\n",
    "    selector = SelectKBest(mutual_info_classif, k=k)\n",
    "    \n",
    "    # Get the scores\n",
    "    selector.fit(X, y)\n",
    "    scores = selector.scores_\n",
    "\n",
    "    df_scores = pd.DataFrame(scores, columns=[\"Score\"], index=X.columns)\n",
    "    df_scores = df_scores.sort_values(by=\"Score\", ascending=False)\n",
    "\n",
    "    # keep the top k features of the df DataFrame\n",
    "    top_score_feature_X = X[df_scores.index[:k]]\n",
    "    # top_score_feature_df doesn't have the label column\n",
    "\n",
    "\n",
    "    # Plot the scores\n",
    "    # import matplotlib.pyplot as plt\n",
    "\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    # plt.barh(df_scores.index, df_scores[\"Score\"])\n",
    "    # plt.xlabel('Score')\n",
    "    # plt.ylabel('Feature')\n",
    "    # plt.title('Feature Importance')\n",
    "    # plt.show()\n",
    "    \n",
    "    return top_score_feature_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression:\n",
    "    \"\"\"Custom Logistic Regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate : float, default=0.01\n",
    "        Learning rate for gradient descent.\n",
    "\n",
    "    num_iterations : int, default=1000\n",
    "        Number of iterations for gradient descent.\n",
    "\n",
    "    verbose : bool, default=False\n",
    "        Print the progress of training if True.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    weights : 1d-array\n",
    "        Weights after fitting the model.\n",
    "\n",
    "    bias : float\n",
    "        Bias after fitting the model.\n",
    "\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    y_pred_class : 1d-array\n",
    "        Predictions using the trained model.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.verbose = verbose\n",
    "        self.weights = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        tmp = np.array(z, dtype=np.float64)\n",
    "        return 1 / (1 + np.exp(-tmp))\n",
    "    \n",
    "    def normailze(self, X):\n",
    "        \"\"\"Normalize the dataset X\"\"\"\n",
    "        tmp = np.array(X, dtype=np.float64)\n",
    "        return (tmp - np.mean(tmp, axis=0)) / np.std(tmp, axis=0)\n",
    "\n",
    "    def fit(self, X, y, early_stop_threshold = 0):\n",
    "        \"\"\"Fit the model using training data\"\"\"\n",
    "\n",
    "        X = self.normailze(X)\n",
    "        ones = np.ones((X.shape[0], 1)) # add a column of ones to X \n",
    "        X = np.concatenate((ones, X), axis=1)\n",
    "        # Initialize the parameters\n",
    "        num_samples, num_features = X.shape\n",
    "        # print(\"num samples: \", num_samples)\n",
    "        # print(\"num features: \", num_features)\n",
    "\n",
    "        self.weights = np.zeros(num_features)\n",
    "        \n",
    "\n",
    "        # Gradient descent\n",
    "        from tqdm import tqdm\n",
    "        for i in tqdm(range(self.num_iterations)):\n",
    "\n",
    "            linear_model = np.dot(X, self.weights)      # y hat = Xw\n",
    "            \n",
    "            # Predictions using sigmoid function\n",
    "            y_pred = self.sigmoid(linear_model)\n",
    "\n",
    "            # Compute error\n",
    "            error = -1.0 * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "            \n",
    "            # Early terminate Gradient Descent if error in the training set becomes < early_stop_threshold\n",
    "            if error < early_stop_threshold:\n",
    "                print(f'Early stopping at Iteration: {i}')\n",
    "                break\n",
    "        \n",
    "\n",
    "            # Compute gradients\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (y_pred - y))\n",
    "            # print (dw)\n",
    "\n",
    "            # Update parameters\n",
    "            self.weights = self.weights - self.learning_rate * dw     # w = w - alpha * dw\n",
    "\n",
    "          \n",
    "\n",
    "    def predict(self, X):\n",
    "        X= self.normailze(X)\n",
    "        ones = np.ones((X.shape[0], 1)) # add a column of ones to X \n",
    "        X = np.concatenate((ones, X), axis=1)\n",
    "        \"\"\"Predict using the trained model\"\"\"\n",
    "        linear_model = np.dot(X, self.weights)      # y hat = Xw\n",
    "        y_pred = self.sigmoid(linear_model)\n",
    "        \n",
    "        y_pred_classified = [1 if i >= 0.5 else 0 for i in y_pred]\n",
    "        # make this into a numpy array\n",
    "        y_pred_classified = np.array(y_pred_classified)\n",
    "        \n",
    "        return y_pred_classified\n",
    "\n",
    "        \n",
    "    def performance(self, y_test, y_pred):\n",
    "        \"\"\"Compute performance\"\"\"\n",
    "\n",
    "        FP = 0\n",
    "        TP = 0\n",
    "        FN = 0\n",
    "        TN = 0\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 1 and y_test[i] == 0:\n",
    "                FP += 1\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 1 and y_test[i] == 1:\n",
    "                TP += 1\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 0 and y_test[i] == 1:\n",
    "                FN += 1\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 0 and y_test[i] == 0:\n",
    "                TN += 1\n",
    "\n",
    "        accuracy = (TP + TN) / len(y_pred)\n",
    "        sensitivity = TP / max( (TP + FN), 1e-10)\n",
    "        precision = TP / max( (TP + FP), 1e-10)\n",
    "        specificity = TN / max( (TN + FP), 1e-10)\n",
    "        false_discovery_rate = FP / max( (TP + FP), 1e-10)\n",
    "        f1 = 2 * precision * sensitivity / max((precision + sensitivity), 1e-10)\n",
    "\n",
    "        return accuracy, sensitivity,specificity, precision, false_discovery_rate, f1\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAdaBoostClassifier:\n",
    "    \"\"\"CustomAdaBoost Classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        the number of hypotheses in the ensemble\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    hypotheses : list\n",
    "        List of weak hypotheses.\n",
    "\n",
    "    alphas : list\n",
    "        List of weights of the weak hypotheses.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=10, learning_rate=0.01, num_iterations=1000, early_stop_threshold = 0, verbose=False):\n",
    "        self.k = k\n",
    "        self.hypotheses = None\n",
    "        self.alphas = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.verbose = verbose\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "  \n",
    "    \n",
    "    \n",
    "    def Resample(self, X, y, weights):\n",
    "        \"\"\"Resample the dataset using the weights\"\"\"\n",
    "\n",
    "        # Sample indices from the range of the length of y with replacement, using the weights\n",
    "        indices = np.random.choice(X.shape[0], X.shape[0], replace = True, p=weights)\n",
    "\n",
    "\n",
    "        print (indices)\n",
    "\n",
    "        # Create new dataset using the sampled indices\n",
    "\n",
    "        # print (X.shape)\n",
    "        # print (y.shape)\n",
    "        y_new = y[indices]\n",
    "        X_new = X.iloc[indices]\n",
    "\n",
    "        return X_new, y_new\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model using training data\"\"\"\n",
    "\n",
    "        # Initialize the weights\n",
    "        num_samples = X.shape[0]\n",
    "        weights = np.ones(num_samples) / num_samples\n",
    "\n",
    "        hypotheses = []\n",
    "        alphas = [] # weights of the hypotheses, Z in the pdf\n",
    "\n",
    "       \n",
    "        for _ in range(self.k):\n",
    "            # create a new dataset using the weights\n",
    "            X, y = self.Resample(X, y, weights)\n",
    "\n",
    "            # Train a weak hypothesis using the weights\n",
    "            hypothesis = CustomLogisticRegression(learning_rate=self.learning_rate, \n",
    "                                                    num_iterations=self.num_iterations, \n",
    "                                                    verbose=self.verbose)\n",
    "\n",
    "            hypothesis.fit(X, y, early_stop_threshold=self.early_stop_threshold)      \n",
    "            \n",
    "\n",
    "            # Compute the error\n",
    "            y_pred = hypothesis.predict(X)\n",
    "            print (\"y_pred:\", y_pred)\n",
    "            error = 0\n",
    "            for j in range(num_samples):\n",
    "                if y_pred[j] != y[j]:\n",
    "                    error += weights[j]\n",
    "\n",
    "            # print (\"error:\", error) \n",
    "\n",
    "            if error > 0.5:\n",
    "                print(\"error is greater than 0.5\")\n",
    "                continue\n",
    "\n",
    "            # Update the weights of the samples. Give the correct predicted samples a lower weight\n",
    "            for i in range(num_samples):\n",
    "                if y_pred[i] == y[i]:\n",
    "                    weights[i] *= error / (1 - error)\n",
    "            weights = weights / np.sum(weights)\n",
    "\n",
    "            hypotheses.append(hypothesis)\n",
    "            # Compute weights of the hypothesis\n",
    "            alpha = np.log((1 - error) / max(error, 1e-10))  # Avoid division by zero\n",
    "            alphas.append(alpha)\n",
    "\n",
    "\n",
    "        self.hypotheses = hypotheses\n",
    "\n",
    "        self.alphas = alphas/np.sum(alphas) # Normalize the weights of the hypotheses\n",
    "        print(\"alphas: \", self.alphas)\n",
    "\n",
    "\n",
    "\n",
    "    def predict_ada(self, X):\n",
    "        \"\"\"Predict using the trained model\"\"\"\n",
    "\n",
    "        num_samples = X.shape[0]\n",
    "        y_pred = np.zeros(num_samples)\n",
    "\n",
    "        print(\"hyp 1:\", type(self.hypotheses[0].predict(X)))\n",
    "        print(\"hyp 2:\", type(self.hypotheses[0].predict(X))[0])\n",
    "\n",
    "\n",
    "        for i in range(len(self.hypotheses)):\n",
    "            # weighted majority hypothesis     \n",
    "            y_pred += self.alphas[i] * self.hypotheses[i].predict(X)\n",
    "            # for j in range(num_samples):\n",
    "            #     y_pred[j] += self.alphas[i] * self.hypotheses[i].predict(X.iloc[j].values.reshape(1, -1))\n",
    "            \n",
    "\n",
    "        y_pred_classified = [1 if i >= 0.5 else 0 for i in y_pred] # classifies the data into 0 or 1\n",
    "    \n",
    "        return y_pred_classified\n",
    "        \n",
    "    \n",
    "\n",
    "    def performance(self, y_test, y_pred):\n",
    "        \"\"\"Compute performance\"\"\"\n",
    "\n",
    "        FP = 0\n",
    "        TP = 0\n",
    "        FN = 0\n",
    "        TN = 0\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 1 and y_test[i] == 0:\n",
    "                FP += 1\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 1 and y_test[i] == 1:\n",
    "                TP += 1\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 0 and y_test[i] == 1:\n",
    "                FN += 1\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 0 and y_test[i] == 0:\n",
    "                TN += 1\n",
    "\n",
    "        accuracy = (TP + TN) / len(y_pred)\n",
    "        sensitivity = TP / max( (TP + FN), 1e-10)\n",
    "        precision = TP / max( (TP + FP), 1e-10)\n",
    "        specificity = TN / max( (TN + FP), 1e-10)\n",
    "        false_discovery_rate = FP / max( (TP + FP), 1e-10)\n",
    "        f1 = 2 * precision * sensitivity / max((precision + sensitivity), 1e-10)\n",
    "\n",
    "        return accuracy, sensitivity,specificity, precision, false_discovery_rate, f1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def telco_customer_churn_preprocess(num_cols_keep = 10):\n",
    "    df = pd.read_csv('./data/Telco Customer Churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "    # print(df.head())\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    # errors='coerce' is a good approach to handle non-numeric values by replacing them with NaN.\n",
    "\n",
    "    df.replace(r'^\\s+$', np.nan, regex=True, inplace=True)\n",
    "    # drop customerID column because this will not help in prediction and cause trouble in one hot encoding\n",
    "    df.drop('customerID', axis=1, inplace=True)\n",
    "\n",
    "    label_col = df.columns[-1]\n",
    "\n",
    "\n",
    "    # Split into feature and label data\n",
    "    X = df.drop(columns=[label_col])\n",
    "    y = df[label_col]\n",
    "\n",
    "    numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    # Exclude columns with binary data\n",
    "    numeric_cols = [col for col in numeric_cols if len(X[col].unique()) > 2]\n",
    "    # print(numeric_cols)\n",
    "\n",
    "    categorical_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    # add the binary column back to the list of categorical columns\n",
    "    binary_cols =  [col for col in numeric_cols if len(X[col].unique()) == 2]\n",
    "    categorical_cols.extend(binary_cols)\n",
    "    X = one_hot_encode(X, categorical_cols)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=82)\n",
    "    # random_state is the seed used by the random number generator\n",
    "    # keeping the train and test data separate to avoid data leakage\n",
    "\n",
    "    handle_missing_values(X_train, option='mean')\n",
    "    handle_missing_values(X_test, option='mean')\n",
    "\n",
    "\n",
    "    normalize_numeric_data(X_train, numeric_cols)\n",
    "    normalize_numeric_data(X_test, numeric_cols)\n",
    "\n",
    "    y_train = LabelEncoder().fit_transform(y_train)\n",
    "    y_test = LabelEncoder().fit_transform(y_test)\n",
    "\n",
    "    X_train = get_top_score_feature_df(X_train, y_train, k=num_cols_keep)\n",
    "\n",
    "    # keep only the top k features in the test data from the train data\n",
    "    X_test = X_test[X_train.columns]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit Card Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_default_preprocessing(num_cols_keep = 25):\n",
    "    df_credit = pd.read_csv('./data/Credit Card Fraud Detection/creditcard.csv')\n",
    "\n",
    "    df_credit.replace(r'^\\s+$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "    # take all the positive samples \n",
    "    df_credit_pos = df_credit[df_credit['Class'] == 1]\n",
    "    # randomly sample 20000 of negative samples\n",
    "    df_credit_neg = df_credit[df_credit['Class'] == 0].sample(n=20000, random_state=82)\n",
    "\n",
    "    # combine the positive and negative samples\n",
    "    df_credit = pd.concat([df_credit_pos, df_credit_neg])\n",
    "\n",
    "\n",
    "    # drop the time column because it is not useful for prediction\n",
    "    df_credit.drop('Time', axis=1, inplace=True)\n",
    "\n",
    "    label_col_credit = df_credit.columns[-1]\n",
    "\n",
    "    # Split into feature and label data\n",
    "    X = df_credit.drop(columns=[label_col_credit])\n",
    "    y = df_credit[label_col_credit]\n",
    "\n",
    "    numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    # Exclude columns with binary data\n",
    "    numeric_cols = [col for col in numeric_cols if len(X[col].unique()) > 2]\n",
    "    # print(numeric_cols)\n",
    "\n",
    "    categorical_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    # add the binary column back to the list of categorical columns\n",
    "    binary_cols =  [col for col in numeric_cols if len(X[col].unique()) == 2]\n",
    "    categorical_cols.extend(binary_cols)\n",
    "    X = one_hot_encode(X, categorical_cols)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=82)\n",
    "    # random_state is the seed used by the random number generator\n",
    "    # keeping the train and test data separate to avoid data leakage\n",
    "\n",
    "    handle_missing_values(X_train, option='mean')\n",
    "    handle_missing_values(X_test, option='mean')\n",
    "\n",
    "\n",
    "    normalize_numeric_data(X_train, numeric_cols)\n",
    "    normalize_numeric_data(X_test, numeric_cols)\n",
    "\n",
    "    y_train = LabelEncoder().fit_transform(y_train)\n",
    "    y_test = LabelEncoder().fit_transform(y_test)\n",
    "\n",
    "    X_train = get_top_score_feature_df(X_train, y_train, k=num_cols_keep)\n",
    "\n",
    "    # keep only the top k features in the test data from the train data\n",
    "    X_test = X_test[X_train.columns]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adult data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adult_income_preprocessing(num_cols_keep=30):\n",
    "    col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship',\n",
    "                 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "    train_data = pd.read_csv('./data/adult/adult.data', names=col_names)\n",
    "    test_data = pd.read_csv('./data/adult/adult.test', names=col_names, skiprows=1)\n",
    "\n",
    "    # drop the fnlwgt column because it is not useful for prediction\n",
    "    train_data.drop('fnlwgt', axis=1, inplace=True)\n",
    "\n",
    "    # drop the education column because it is redundant with education-num\n",
    "    train_data.drop('education', axis=1, inplace=True)\n",
    "\n",
    "    # drop the country column because it is not useful for prediction\n",
    "    train_data.drop('native-country', axis=1, inplace=True)\n",
    "\n",
    "    label_col = train_data.columns[-1]\n",
    "\n",
    "    # Split into feature and label data\n",
    "    X_train = train_data.drop(columns=[label_col])\n",
    "    y_train = train_data[label_col]\n",
    "\n",
    "    X_test = test_data.drop(columns=[label_col])\n",
    "    y_test = test_data[label_col]\n",
    "\n",
    "    # modifying values in specific columns\n",
    "    y_train.replace({' >50K': 1, ' <=50K': 0}, inplace=True)\n",
    "    y_test.replace({' >50K.': 1, ' <=50K.': 0}, inplace=True)\n",
    "\n",
    "    \n",
    "    X_train.replace(' ?', np.nan, inplace=True)\n",
    "    X_test.replace(' ?', np.nan, inplace=True)\n",
    "\n",
    "    handle_missing_values(X_train, option='mode')\n",
    "    handle_missing_values(X_test, option='mode')\n",
    "\n",
    "    numeric_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "    normalize_numeric_data(X_train, numeric_cols)\n",
    "    normalize_numeric_data(X_test, numeric_cols)\n",
    "    \n",
    "\n",
    "    X_train = pd.get_dummies(X_train)\n",
    "    X_test = pd.get_dummies(X_test)\n",
    "\n",
    "    y_train = LabelEncoder().fit_transform(y_train)\n",
    "    y_test = LabelEncoder().fit_transform(y_test)\n",
    "\n",
    "\n",
    "\n",
    "    X_train = get_top_score_feature_df(X_train, y_train, k=num_cols_keep)\n",
    "    X_test = X_test[X_train.columns]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are missing values in the dataset.\n",
      "Missing values per column:\n",
      "workclass     1836\n",
      "occupation    1843\n",
      "dtype: int64\n",
      "Missing values per column are fixed\n",
      "\n",
      "There are missing values in the dataset.\n",
      "Missing values per column:\n",
      "workclass         963\n",
      "occupation        966\n",
      "native-country    274\n",
      "dtype: int64\n",
      "Missing values per column are fixed\n",
      "\n",
      "k: 30\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = adult_income_preprocessing(num_cols_keep=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_regression(dataset_name):\n",
    "    np.random.seed(82)\n",
    "\n",
    "    if dataset_name == 'telco':\n",
    "        X_train, X_test, y_train, y_test = telco_customer_churn_preprocess()\n",
    "        \n",
    "\n",
    "    if dataset_name == 'credit':\n",
    "        X_train, X_test, y_train, y_test = credit_card_default_preprocessing()\n",
    "        \n",
    "\n",
    "    if dataset_name == 'adult':\n",
    "        X_train, X_test, y_train, y_test = adult_income_preprocessing()\n",
    "\n",
    "\n",
    "    model = CustomLogisticRegression(learning_rate=0.01, num_iterations=5000, verbose=False)\n",
    "\n",
    " \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Logistic Regression Test\")\n",
    "    accuracy, sensitivity, specificity, precision, false_discovery_rate, f1 = model.performance(y_test, y_pred)\n",
    "    print(\"Accuracy: \", accuracy * 100)\n",
    "    print(\"Sensitivity: \", sensitivity * 100)\n",
    "    print(\"Specificity: \", specificity * 100)\n",
    "    print(\"Precision: \", precision * 100)\n",
    "    print(\"False Discovery Rate: \", false_discovery_rate * 100)\n",
    "    print(\"F1: \", f1 * 100)\n",
    "\n",
    "    # save this report in a csv file\n",
    "    report = pd.DataFrame(columns=['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'False Discovery Rate', 'F1'], index=['Logistic Regression Test'])\n",
    "    report.loc['Logistic Regression Test'] = [accuracy, sensitivity, specificity, precision, false_discovery_rate, f1]\n",
    "    report.transpose()\n",
    "    report.to_csv(f'./logistic_regression_{dataset_name}_report.csv')\n",
    "\n",
    "\n",
    "    y_pred = model.predict(X_train)\n",
    "\n",
    "    print(\"Logistic Regression Train\")\n",
    "    accuracy, sensitivity, specificity, precision, false_discovery_rate, f1 = model.performance(y_train, y_pred)\n",
    "    print(\"Accuracy: \", accuracy * 100)\n",
    "    print(\"Sensitivity: \", sensitivity * 100)\n",
    "    print(\"Specificity: \", specificity * 100)\n",
    "    print(\"Precision: \", precision * 100)\n",
    "    print(\"False Discovery Rate: \", false_discovery_rate * 100)\n",
    "    print(\"F1: \", f1 * 100)\n",
    "          \n",
    "    report = pd.DataFrame(columns=['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'False Discovery Rate', 'F1'], index=['Logistic Regression Train'])\n",
    "    report.loc['Logistic Regression Train'] = [accuracy, sensitivity, specificity, precision, false_discovery_rate, f1]\n",
    "    report.transpose()\n",
    "    report.to_csv(f'./logistic_regression_{dataset_name}_report.csv', mode='a')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_adaboost(dataset_name, boosting_round = 5):\n",
    "    np.random.seed(82)\n",
    "    if dataset_name == 'telco':\n",
    "        X_train, X_test, y_train, y_test = telco_customer_churn_preprocess()\n",
    "        \n",
    "\n",
    "    if dataset_name == 'credit':\n",
    "        X_train, X_test, y_train, y_test = credit_card_default_preprocessing()\n",
    "        \n",
    "\n",
    "    if dataset_name == 'adult':\n",
    "        X_train, X_test, y_train, y_test = adult_income_preprocessing()\n",
    "\n",
    "\n",
    "    model = CustomAdaBoostClassifier(\n",
    "        k=boosting_round,\n",
    "        learning_rate=0.01,\n",
    "        num_iterations=1000,\n",
    "        early_stop_threshold=0.5,\n",
    "        verbose=False,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_ada(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Adaboost Test\")\n",
    "    accuracy, sensitivity, specificity, precision, false_discovery_rate, f1 = model.performance(y_test, y_pred)\n",
    "    print(\"Accuracy: \", accuracy * 100)\n",
    "    print(\"Sensitivity: \", sensitivity * 100)\n",
    "    print(\"Specificity: \", specificity * 100)\n",
    "    print(\"Precision: \", precision * 100)\n",
    "    print(\"False Discovery Rate: \", false_discovery_rate * 100)\n",
    "    print(\"F1: \", f1 * 100)\n",
    "\n",
    "    # save this report in a csv file\n",
    "    report = pd.DataFrame(columns=['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'False Discovery Rate', 'F1'], index=['Adaboost Test'])\n",
    "    report.loc['Adaboost Test'] = [accuracy, sensitivity, specificity, precision, false_discovery_rate, f1]\n",
    "    report.transpose()\n",
    "    report.to_csv(f'./adaboost_{dataset_name}_report.csv')\n",
    "\n",
    "\n",
    "    y_pred = model.predict_ada(X_train)\n",
    "\n",
    "    print(\"Adaboost Train\")\n",
    "    accuracy, sensitivity, specificity, precision, false_discovery_rate, f1 = model.performance(y_train, y_pred)\n",
    "    print(\"Accuracy: \", accuracy * 100)\n",
    "    print(\"Sensitivity: \", sensitivity * 100)\n",
    "    print(\"Specificity: \", specificity * 100)\n",
    "    print(\"Precision: \", precision * 100)\n",
    "    print(\"False Discovery Rate: \", false_discovery_rate * 100)\n",
    "    print(\"F1: \", f1 * 100)\n",
    "          \n",
    "    report = pd.DataFrame(columns=['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'False Discovery Rate', 'F1'], index=['Adaboost Train'])\n",
    "    report.loc['Adaboost Train'] = [accuracy, sensitivity, specificity, precision, false_discovery_rate, f1]\n",
    "    report.transpose()\n",
    "    report.to_csv(f'./adaboost_{dataset_name}_report.csv', mode='a')\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are missing values in the dataset.\n",
      "Missing values per column:\n",
      "workclass     1836\n",
      "occupation    1843\n",
      "dtype: int64\n",
      "Missing values per column are fixed\n",
      "\n",
      "There are missing values in the dataset.\n",
      "Missing values per column:\n",
      "workclass         963\n",
      "occupation        966\n",
      "native-country    274\n",
      "dtype: int64\n",
      "Missing values per column are fixed\n",
      "\n",
      "k: 30\n",
      "[ 4918  9874 27474 ... 12327  1255  4686]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:32<00:00, 10.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: [1 0 0 ... 0 0 0]\n",
      "[21964 24852 15932 ... 26731 21936 19252]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:43<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: [0 1 1 ... 1 0 1]\n",
      "[ 6261 18902 20078 ... 12689 25532  3346]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:36<00:00, 10.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: [1 1 0 ... 0 0 0]\n",
      "[15568 27234 26632 ... 29222 22136 26821]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:35<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: [1 1 1 ... 0 1 1]\n",
      "[24116 22312  9380 ... 30734 22235 14971]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:40<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: [1 0 0 ... 0 0 0]\n",
      "alphas:  [0.33426966 0.2870561  0.13232518 0.14317351 0.10317555]\n",
      "hyp 1: <class 'numpy.ndarray'>\n",
      "hyp 2: numpy.ndarray[0]\n",
      "Adaboost Test\n",
      "Accuracy:  78.47183833916836\n",
      "Sensitivity:  20.202808112324494\n",
      "Specificity:  96.49376759147567\n",
      "Precision:  64.0560593569662\n",
      "False Discovery Rate:  35.9439406430338\n",
      "F1:  30.717533109310136\n",
      "hyp 1: <class 'numpy.ndarray'>\n",
      "hyp 2: numpy.ndarray[0]\n",
      "Adaboost Train\n",
      "Accuracy:  78.17634593532141\n",
      "Sensitivity:  19.971942354291546\n",
      "Specificity:  96.6383495145631\n",
      "Precision:  65.33166458072591\n",
      "False Discovery Rate:  34.668335419274094\n",
      "F1:  30.591912482906817\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 2):\n",
    "    # run_adaboost('telco', i*5)\n",
    "    # run_adaboost('credit', i*5)\n",
    "    run_adaboost('adult', i*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are missing values in the dataset.\n",
      "Missing values per column:\n",
      "workclass     1836\n",
      "occupation    1843\n",
      "dtype: int64\n",
      "Missing values per column are fixed\n",
      "\n",
      "There are missing values in the dataset.\n",
      "Missing values per column:\n",
      "workclass         963\n",
      "occupation        966\n",
      "native-country    274\n",
      "dtype: int64\n",
      "Missing values per column are fixed\n",
      "\n",
      "k: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:16<00:00, 312.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test\n",
      "Accuracy:  84.85965235550643\n",
      "Precision:  72.51385718943592\n",
      "Recall:  57.8263130525221\n",
      "F1:  64.34254303486185\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_logistic_regression('adult')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
