{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, option : str='mean'):\n",
    "    \"\"\"Fill missing values in data frame in place\"\"\"\n",
    "    \n",
    "    missing_values = df.isna().sum()\n",
    "    # print(missing_values)\n",
    "\n",
    "    # Check if there are any missing values\n",
    "    if missing_values.any():\n",
    "        print(\"There are missing values in the dataset.\")\n",
    "        # Display the count of missing values for each column\n",
    "        # find the columns with missing values\n",
    "        columns_with_missing_values = df.columns[missing_values > 0]\n",
    "        print(\"Missing values per column:\")\n",
    "        print(missing_values[columns_with_missing_values])\n",
    "\n",
    "        # fix the missing values\n",
    "        # we can use df.replace(to_replace=' ', value=-1) to replace all missing values with -1\n",
    "        for column in columns_with_missing_values:\n",
    "\n",
    "            if option == 'drop':\n",
    "                df.dropna()\n",
    "            elif option == 'mean':\n",
    "                df[column].fillna(df[column].mean(), inplace=True)\n",
    "            elif option == 'median':\n",
    "                df[column].fillna(df[column].median(), inplace=True)\n",
    "            elif option == 'mode':\n",
    "                df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "            elif option == 'linear':\n",
    "                df[column].interpolate(method='linear', limit_direction='forward', inplace=True)\n",
    "            elif option == 'quadratic':\n",
    "                df[column].interpolate(method='quadratic', limit_direction='forward', inplace=True)\n",
    "            elif option == 'cubic':\n",
    "                df[column].interpolate(method='cubic', limit_direction='forward', inplace=True)\n",
    "            elif option == 'spline':\n",
    "                df[column].interpolate(method='spline', order=3, limit_direction='forward', inplace=True)\n",
    "        \n",
    "        print(\"Missing values per column are fixed\\n\")\n",
    "\n",
    "    else:\n",
    "        print(\"There are no missing values in the dataset.\\n\")\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_numeric_data(df, numeric_cols):\n",
    "    \"\"\"Normalize numeric columns in dataframe in place\"\"\"\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "    # print(numeric_cols)\n",
    "    scaler = MinMaxScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_encode(df, categorical_cols):\n",
    "    \"\"\"One hot encode categorical columns in dataframee\"\"\"\n",
    "    one_hot_encoded_data  = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    categorical_cols = one_hot_encoded_data.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    label_encode(one_hot_encoded_data, categorical_cols)\n",
    "\n",
    "    return one_hot_encoded_data \n",
    "\n",
    "    \n",
    "def label_encode(df, categorical_cols):\n",
    "    \"\"\"Label encode categorical columns in dataframe in place\"\"\"\n",
    "\n",
    "    # print(categorical_cols)\n",
    "    \n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        label_encoders[col] = LabelEncoder()\n",
    "        df[col] = label_encoders[col].fit_transform(df[col])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_score_feature_df(X, y, k=15):\n",
    "    \"\"\"Get the top k features with the highest score\n",
    "        Return a new dataframe with the top k features\n",
    "    \"\"\"\n",
    "    # todo: need to implement this function\n",
    "    from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "    print (\"k:\",k)\n",
    "\n",
    "    selector = SelectKBest(mutual_info_classif, k=k)\n",
    "    \n",
    "    # Get the scores\n",
    "    selector.fit(X, y)\n",
    "    scores = selector.scores_\n",
    "\n",
    "    df_scores = pd.DataFrame(scores, columns=[\"Score\"], index=X.columns)\n",
    "    df_scores = df_scores.sort_values(by=\"Score\", ascending=False)\n",
    "\n",
    "    # keep the top k features of the df DataFrame\n",
    "    top_score_feature_X = X[df_scores.index[:k]]\n",
    "    # top_score_feature_df doesn't have the label column\n",
    "\n",
    "\n",
    "    # Plot the scores\n",
    "    # import matplotlib.pyplot as plt\n",
    "\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    # plt.barh(df_scores.index, df_scores[\"Score\"])\n",
    "    # plt.xlabel('Score')\n",
    "    # plt.ylabel('Feature')\n",
    "    # plt.title('Feature Importance')\n",
    "    # plt.show()\n",
    "    \n",
    "    return top_score_feature_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logisticRegressionLibrary(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Logistic Regression\"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Logistic Regression\")\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, y_pred)* 100)\n",
    "    print(\"Precision: \", precision_score(y_test, y_pred) * 100)\n",
    "    print(\"Recall: \", recall_score(y_test, y_pred) * 100)\n",
    "    print(\"F1: \", f1_score(y_test, y_pred) * 100)\n",
    "    print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression:\n",
    "    \"\"\"Custom Logistic Regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate : float, default=0.01\n",
    "        Learning rate for gradient descent.\n",
    "\n",
    "    num_iterations : int, default=1000\n",
    "        Number of iterations for gradient descent.\n",
    "\n",
    "    verbose : bool, default=False\n",
    "        Print the progress of training if True.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    weights : 1d-array\n",
    "        Weights after fitting the model.\n",
    "\n",
    "    bias : float\n",
    "        Bias after fitting the model.\n",
    "\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    y_pred_class : 1d-array\n",
    "        Predictions using the trained model.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.verbose = verbose\n",
    "        self.weights = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y, early_stop_threshold = 0):\n",
    "        \"\"\"Fit the model using training data\"\"\"\n",
    "\n",
    "        \n",
    "\n",
    "        # Initialize the parameters\n",
    "        num_samples, num_features = X.shape\n",
    "        # print(\"num samples: \", num_samples)\n",
    "        # print(\"num features: \", num_features)\n",
    "        \n",
    "        # Initialize the weights randomly but reproducibly\n",
    "        np.random.seed(82)\n",
    "        self.weights = np.random.rand(num_features)\n",
    "\n",
    "        # Gradient descent\n",
    "        from tqdm import tqdm\n",
    "        for i in tqdm(range(self.num_iterations)):\n",
    "\n",
    "            linear_model = np.dot(X, self.weights)      # y hat = Xw\n",
    "            \n",
    "            # Predictions using sigmoid function\n",
    "            y_pred = self.sigmoid(linear_model)\n",
    "\n",
    "            # Calculate how many samples are misclassified \n",
    "            y_pred_classified = [1 if i >= 0.5 else 0 for i in y_pred] # classifies the data into 0 or 1\n",
    "            num_correct_samples = sum([1 if y_pred_classified[i] == y[i] else 0 for i in range(len(y))])\n",
    "            \n",
    "            # print(\"num correct samples: \", num_correct_samples)\n",
    "            error = 1 - num_correct_samples / num_samples\n",
    "            \n",
    "            # Early terminate Gradient Descent if error in the training set becomes < early_stop_threshold\n",
    "            if error < early_stop_threshold and early_stop_threshold != 0:\n",
    "                print(f'Early stopping at Iteration: {i}')\n",
    "                break\n",
    "        \n",
    "\n",
    "            # Compute gradients\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (y_pred - y))\n",
    "            # print (dw)\n",
    "\n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw     # w = w - alpha * dw\n",
    "\n",
    "            if self.verbose and i % 100 == 0:\n",
    "                print(f'Iteration {i}, weights: {self.weights}, bias: {self.bias}')\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using the trained model\"\"\"\n",
    "        linear_model = np.dot(X, self.weights)      # y hat = Xw\n",
    "        y_pred = self.sigmoid(linear_model)\n",
    "        \n",
    "        y_pred_classified = [1 if i >= 0.5 else 0 for i in y_pred]\n",
    "        # make this into a numpy array\n",
    "        y_pred_classified = np.array(y_pred_classified)\n",
    "        \n",
    "        return y_pred_classified\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAdaBoostClassifier:\n",
    "    \"\"\"CustomAdaBoost Classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        the number of hypotheses in the ensemble\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    hypotheses : list\n",
    "        List of weak hypotheses.\n",
    "\n",
    "    alphas : list\n",
    "        List of weights of the weak hypotheses.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=10, learning_rate=0.01, num_iterations=1000, early_stop_threshold = 0, verbose=False):\n",
    "        self.k = k\n",
    "        self.hypotheses = None\n",
    "        self.alphas = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.verbose = verbose\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "  \n",
    "    \n",
    "    \n",
    "    def Resample(self, X, y, weights):\n",
    "        \"\"\"Resample the dataset using the weights\"\"\"\n",
    "\n",
    "        # Sample indices from the range of the length of y with replacement, using the weights\n",
    "        indices = np.random.choice(X.shape[0], X.shape[0], replace = True, p=weights)\n",
    "\n",
    "\n",
    "        print (indices)\n",
    "\n",
    "        # Create new dataset using the sampled indices\n",
    "\n",
    "        # print (X.shape)\n",
    "        # print (y.shape)\n",
    "        y_new = y[indices]\n",
    "        X_new = X.iloc[indices]\n",
    "\n",
    "        return X_new, y_new\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model using training data\"\"\"\n",
    "\n",
    "        # Initialize the weights\n",
    "        num_samples = X.shape[0]\n",
    "        weights = np.ones(num_samples) / num_samples\n",
    "\n",
    "        hypotheses = []\n",
    "        alphas = [] # weights of the hypotheses, Z in the pdf\n",
    "\n",
    "       \n",
    "        for _ in range(self.k):\n",
    "            # create a new dataset using the weights\n",
    "            X, y = self.Resample(X, y, weights)\n",
    "\n",
    "            # Train a weak hypothesis using the weights\n",
    "            hypothesis = CustomLogisticRegression(learning_rate=self.learning_rate, \n",
    "                                                    num_iterations=self.num_iterations, \n",
    "                                                    verbose=self.verbose)\n",
    "            \n",
    "            hypothesis.fit(X, y, early_stop_threshold=self.early_stop_threshold)\n",
    "\n",
    "            # Compute the error\n",
    "            y_pred = hypothesis.predict(X)\n",
    "            error = 0\n",
    "            for j in range(len(y_pred)):\n",
    "                if y_pred[j] != y[j]:\n",
    "                    error += weights[j]\n",
    "\n",
    "            # print (\"error:\", error) \n",
    "\n",
    "            if error > 0.5:\n",
    "                print(\"error is greater than 0.5\")\n",
    "                continue\n",
    "\n",
    "            # Update the weights of the samples. Give the correct predicted samples a lower weight\n",
    "            for i in range(len(weights)):\n",
    "                if y_pred[i] == y[i]:\n",
    "                    weights[i] *= error / (1 - error)\n",
    "            weights = weights / np.sum(weights)\n",
    "\n",
    "            hypotheses.append(hypothesis)\n",
    "            # Compute weights of the hypothesis\n",
    "            alpha = np.log((1 - error) / max(error, 1e-10))  # Avoid division by zero\n",
    "            alphas.append(alpha)\n",
    "\n",
    "\n",
    "        self.hypotheses = hypotheses\n",
    "\n",
    "        self.alphas = alphas/np.sum(alphas) # Normalize the weights of the hypotheses\n",
    "        print(\"alphas: \", self.alphas)\n",
    "\n",
    "\n",
    "\n",
    "    def predict_ada(self, X):\n",
    "        \"\"\"Predict using the trained model\"\"\"\n",
    "\n",
    "        num_samples = X.shape[0]\n",
    "        y_pred = np.zeros(num_samples)\n",
    "\n",
    "        print(\"hyp 1:\", type(self.hypotheses[0].predict(X)))\n",
    "        print(\"hyp 2:\", type(self.hypotheses[0].predict(X))[0])\n",
    "\n",
    "\n",
    "        for i in range(len(self.hypotheses)):\n",
    "            # weighted majority hypothesis     \n",
    "            # y_pred += self.alphas[i] * self.hypotheses[i].predict(X)\n",
    "            for j in range(num_samples):\n",
    "                y_pred[j] += self.alphas[i] * self.hypotheses[i].predict(X.iloc[j].values.reshape(1, -1))\n",
    "            \n",
    "\n",
    "        y_pred_classified = [1 if i >= 0.5 else 0 for i in y_pred] # classifies the data into 0 or 1\n",
    "    \n",
    "        return y_pred_classified\n",
    "        \n",
    "    \n",
    "\n",
    "    def performance(self, y_test, y_pred):\n",
    "        \"\"\"Compute performance\"\"\"\n",
    "\n",
    "        FP = 0\n",
    "        TP = 0\n",
    "        FN = 0\n",
    "        TN = 0\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 1 and y_test[i] == 0:\n",
    "                FP += 1\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 1 and y_test[i] == 1:\n",
    "                TP += 1\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 0 and y_test[i] == 1:\n",
    "                FN += 1\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 0 and y_test[i] == 0:\n",
    "                TN += 1\n",
    "\n",
    "        accuracy = (TP + TN) / len(y_pred)\n",
    "        precision = TP / max( (TP + FP), 1e-10)\n",
    "        recall = TP / max((TP + FN), 1e-10)\n",
    "        f1 = 2 * precision * recall / max((precision + recall), 1e-10)\n",
    "\n",
    "        return accuracy, precision, recall, f1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def telco_customer_churn_preprocess(num_cols_keep = 10):\n",
    "    df = pd.read_csv('./data/Telco Customer Churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "    # print(df.head())\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    # errors='coerce' is a good approach to handle non-numeric values by replacing them with NaN.\n",
    "\n",
    "    df.replace(r'^\\s+$', np.nan, regex=True, inplace=True)\n",
    "    # drop customerID column because this will not help in prediction and cause trouble in one hot encoding\n",
    "    df.drop('customerID', axis=1, inplace=True)\n",
    "\n",
    "    label_col = df.columns[-1]\n",
    "\n",
    "\n",
    "    # Split into feature and label data\n",
    "    X = df.drop(columns=[label_col])\n",
    "    y = df[label_col]\n",
    "\n",
    "    numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    # Exclude columns with binary data\n",
    "    numeric_cols = [col for col in numeric_cols if len(X[col].unique()) > 2]\n",
    "    # print(numeric_cols)\n",
    "\n",
    "    categorical_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    # add the binary column back to the list of categorical columns\n",
    "    binary_cols =  [col for col in numeric_cols if len(X[col].unique()) == 2]\n",
    "    categorical_cols.extend(binary_cols)\n",
    "    X = one_hot_encode(X, categorical_cols)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=82)\n",
    "    # random_state is the seed used by the random number generator\n",
    "    # keeping the train and test data separate to avoid data leakage\n",
    "\n",
    "    handle_missing_values(X_train, option='mean')\n",
    "    handle_missing_values(X_test, option='mean')\n",
    "\n",
    "\n",
    "    normalize_numeric_data(X_train, numeric_cols)\n",
    "    normalize_numeric_data(X_test, numeric_cols)\n",
    "\n",
    "    y_train = LabelEncoder().fit_transform(y_train)\n",
    "    y_test = LabelEncoder().fit_transform(y_test)\n",
    "\n",
    "    X_train = get_top_score_feature_df(X_train, y_train, k=num_cols_keep)\n",
    "\n",
    "    # keep only the top k features in the test data from the train data\n",
    "    X_test = X_test[X_train.columns]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are missing values in the dataset.\n",
      "Missing values per column:\n",
      "TotalCharges    8\n",
      "dtype: int64\n",
      "Missing values per column are fixed\n",
      "\n",
      "There are missing values in the dataset.\n",
      "Missing values per column:\n",
      "TotalCharges    3\n",
      "dtype: int64\n",
      "Missing values per column are fixed\n",
      "\n",
      "k: 25\n",
      "[2345 4188 4041 ... 4970 1029 3344]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:13<00:00, 74.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  58 2428  293 ... 2719 4180 5504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 94.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  60 2429  302 ... 2720 4192 5508]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 139.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is greater than 0.5\n",
      "[  60 2429  302 ... 2720 4192 5508]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 146.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is greater than 0.5\n",
      "[  60 2429  302 ... 2720 4192 5508]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 145.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is greater than 0.5\n",
      "[  60 2429  302 ... 2720 4192 5508]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 142.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is greater than 0.5\n",
      "[  60 2429  302 ... 2720 4192 5508]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 152.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is greater than 0.5\n",
      "[  60 2429  302 ... 2720 4192 5508]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 151.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is greater than 0.5\n",
      "[  60 2429  302 ... 2720 4192 5508]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 152.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is greater than 0.5\n",
      "[  60 2429  302 ... 2720 4192 5508]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 142.39it/s]\n",
      "C:\\Users\\Pridesys\\AppData\\Local\\Temp\\ipykernel_26244\\2705528606.py:116: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y_pred[j] += self.alphas[i] * self.hypotheses[i].predict(X.iloc[j].values.reshape(1, -1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphas:  [0.64163237 0.28783019 0.07053744]\n",
      "hyp 1: <class 'numpy.ndarray'>\n",
      "hyp 2: numpy.ndarray[0]\n",
      "AdaBoost Classifier\n",
      "Accuracy:  77.35982966643009\n",
      "Precision:  65.94594594594595\n",
      "Recall:  32.27513227513227\n",
      "F1:  43.3392539964476\n",
      "\n",
      "\n",
      "\n",
      "Accuracy:  77.35982966643009\n",
      "Precision:  65.94594594594595\n",
      "Recall:  32.27513227513227\n",
      "F1:  43.3392539964476\n",
      "Confusion Matrix: \n",
      " [[968  63]\n",
      " [256 122]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = telco_customer_churn_preprocess(num_cols_keep=25)\n",
    "\n",
    "model = CustomAdaBoostClassifier(\n",
    "    k=10,\n",
    "    learning_rate=0.01,\n",
    "    num_iterations=1000,\n",
    "    early_stop_threshold=0.1,\n",
    "    verbose=False,\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict_ada(X_test)\n",
    "\n",
    "print(\"AdaBoost Classifier\")\n",
    "accuracy, precision, recall, f1 = model.performance(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy * 100)\n",
    "print(\"Precision: \", precision * 100)\n",
    "print(\"Recall: \", recall * 100)\n",
    "print(\"F1: \", f1 * 100)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred)* 100)\n",
    "print(\"Precision: \", precision_score(y_test, y_pred) * 100)\n",
    "print(\"Recall: \", recall_score(y_test, y_pred) * 100)\n",
    "print(\"F1: \", f1_score(y_test, y_pred) * 100)\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:09<00:00, 100.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy:  79.63094393186657\n",
      "Precision:  65.11627906976744\n",
      "Recall:  51.85185185185185\n",
      "F1:  57.731958762886606\n",
      "Confusion Matrix: \n",
      " [[926 105]\n",
      " [182 196]]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "customRegressionModel = CustomLogisticRegression(learning_rate=0.1, num_iterations=1000, verbose=False)\n",
    "customRegressionModel.fit(X_train, y_train, early_stop_threshold=0.1)\n",
    "y_pred = customRegressionModel.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred)* 100)\n",
    "print(\"Precision: \", precision_score(y_test, y_pred) * 100)\n",
    "print(\"Recall: \", recall_score(y_test, y_pred) * 100)\n",
    "print(\"F1: \", f1_score(y_test, y_pred) * 100)\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adult Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adult_data_preprocessing():\n",
    "    df = pd.read_csv('./data/adult/adult.data', header=None)\n",
    "    print(df.head())\n",
    "    print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0                  1       2           3   4                    5   \\\n",
      "0  39          State-gov   77516   Bachelors  13        Never-married   \n",
      "1  50   Self-emp-not-inc   83311   Bachelors  13   Married-civ-spouse   \n",
      "2  38            Private  215646     HS-grad   9             Divorced   \n",
      "3  53            Private  234721        11th   7   Married-civ-spouse   \n",
      "4  28            Private  338409   Bachelors  13   Married-civ-spouse   \n",
      "\n",
      "                   6               7       8        9     10  11  12  \\\n",
      "0        Adm-clerical   Not-in-family   White     Male  2174   0  40   \n",
      "1     Exec-managerial         Husband   White     Male     0   0  13   \n",
      "2   Handlers-cleaners   Not-in-family   White     Male     0   0  40   \n",
      "3   Handlers-cleaners         Husband   Black     Male     0   0  40   \n",
      "4      Prof-specialty            Wife   Black   Female     0   0  40   \n",
      "\n",
      "               13      14  \n",
      "0   United-States   <=50K  \n",
      "1   United-States   <=50K  \n",
      "2   United-States   <=50K  \n",
      "3   United-States   <=50K  \n",
      "4            Cuba   <=50K  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       32561 non-null  int64 \n",
      " 1   1       32561 non-null  object\n",
      " 2   2       32561 non-null  int64 \n",
      " 3   3       32561 non-null  object\n",
      " 4   4       32561 non-null  int64 \n",
      " 5   5       32561 non-null  object\n",
      " 6   6       32561 non-null  object\n",
      " 7   7       32561 non-null  object\n",
      " 8   8       32561 non-null  object\n",
      " 9   9       32561 non-null  object\n",
      " 10  10      32561 non-null  int64 \n",
      " 11  11      32561 non-null  int64 \n",
      " 12  12      32561 non-null  int64 \n",
      " 13  13      32561 non-null  object\n",
      " 14  14      32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "adult_data_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit Card Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_default_preprocessing():\n",
    "    df = pd.read_csv('./data/Credit Card Fraud Detection/creditcard.csv', header=1)\n",
    "    print(df.head())\n",
    "    print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit = pd.read_csv('./data/Credit Card Fraud Detection/creditcard.csv')\n",
    "\n",
    "df_credit.replace(r'^\\s+$', np.nan, regex=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no missing values in the dataset.\n",
      "\n",
      "There are no missing values in the dataset.\n",
      "\n",
      "k: 15\n"
     ]
    }
   ],
   "source": [
    "label_col_credit = df_credit.columns[-1]\n",
    "\n",
    "\n",
    "# Split into feature and label data\n",
    "X = df_credit.drop(columns=[label_col_credit])\n",
    "y = df_credit[label_col_credit]\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "# Exclude columns with binary data\n",
    "numeric_cols = [col for col in numeric_cols if len(X[col].unique()) > 2]\n",
    "# print(numeric_cols)\n",
    "\n",
    "categorical_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "# add the binary column back to the list of categorical columns\n",
    "binary_cols =  [col for col in numeric_cols if len(X[col].unique()) == 2]\n",
    "categorical_cols.extend(binary_cols)\n",
    "X = one_hot_encode(X, categorical_cols)\n",
    "\n",
    "# use a smaller data set\n",
    "X = X[:50000]\n",
    "y = y[:50000]\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=82)\n",
    "# random_state is the seed used by the random number generator\n",
    "# keeping the train and test data separate to avoid data leakage\n",
    "\n",
    "handle_missing_values(X_train, option='mean')\n",
    "handle_missing_values(X_test, option='mean')\n",
    "\n",
    "\n",
    "normalize_numeric_data(X_train, numeric_cols)\n",
    "normalize_numeric_data(X_test, numeric_cols)\n",
    "\n",
    "y_train = LabelEncoder().fit_transform(y_train)\n",
    "y_test = LabelEncoder().fit_transform(y_test)\n",
    "\n",
    "X_train = get_top_score_feature_df(X_train, y_train, k=15)\n",
    "\n",
    "# keep only the top k features in the test data from the train data\n",
    "X_test = X_test[X_train.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18552 38140 34154 ... 28298 32484 12905]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 115/1000 [00:08<01:05, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at Iteration: 115\n",
      "[32398 27063 21670 ... 28811 38105 35419]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 116/1000 [00:06<00:48, 18.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at Iteration: 116\n",
      "[32273 26759 21079 ... 28623 38108 35311]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 120/1000 [00:05<00:36, 23.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at Iteration: 120\n",
      "[32082 26552 20696 ... 28637 37961 34969]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 139/1000 [00:04<00:30, 27.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at Iteration: 139\n",
      "[30897 25298 20480 ... 27332 37503 34097]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:53<00:00, 18.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30721 24760 19364 ... 27015 37453 33973]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:45<00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30637 24870 19286 ... 27015 37453 33973]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:46<00:00, 21.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30680 24870 19156 ... 27071 37559 34051]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 29.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30728 24880 19172 ... 27071 37559 34078]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:34<00:00, 28.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30779 25061 19256 ... 27252 37559 34097]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:52<00:00, 18.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphas:  [0.17470181 0.18780579 0.16035987 0.17070854 0.12791327 0.04726036\n",
      " 0.05363765 0.03607685 0.01207711 0.02945874]\n",
      "hyp 1: <class 'numpy.ndarray'>\n",
      "hyp 2: numpy.ndarray[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pridesys\\AppData\\Local\\Temp\\ipykernel_26244\\1962145368.py:116: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y_pred[j] += self.alphas[i] * self.hypotheses[i].predict(X.iloc[j].values.reshape(1, -1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier\n",
      "Accuracy:  99.77000000000001\n",
      "Precision:  62.0\n",
      "Recall:  88.57142857142857\n",
      "F1:  72.94117647058823\n",
      "\n",
      "\n",
      "\n",
      "Accuracy:  99.77000000000001\n",
      "Precision:  62.0\n",
      "Recall:  88.57142857142857\n",
      "F1:  72.94117647058823\n",
      "Confusion Matrix: \n",
      " [[9946   19]\n",
      " [   4   31]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = CustomAdaBoostClassifier(\n",
    "    k=10,\n",
    "    learning_rate=0.01,\n",
    "    num_iterations=1000,\n",
    "    early_stop_threshold=0.1,\n",
    "    verbose=False,\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict_ada(X_test)\n",
    "\n",
    "print(\"AdaBoost Classifier\")\n",
    "accuracy, precision, recall, f1 = model.performance(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy * 100)\n",
    "print(\"Precision: \", precision * 100)\n",
    "print(\"Recall: \", recall * 100)\n",
    "print(\"F1: \", f1 * 100)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred)* 100)\n",
    "print(\"Precision: \", precision_score(y_test, y_pred) * 100)\n",
    "print(\"Recall: \", recall_score(y_test, y_pred) * 100)\n",
    "print(\"F1: \", f1_score(y_test, y_pred) * 100)\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
