{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, option : str='mean'):\n",
    "    \"\"\"Fill missing values in data frame in place\"\"\"\n",
    "    \n",
    "    missing_values = df.isna().sum()\n",
    "    # print(missing_values)\n",
    "\n",
    "    # Check if there are any missing values\n",
    "    if missing_values.any():\n",
    "        print(\"There are missing values in the dataset.\")\n",
    "        # Display the count of missing values for each column\n",
    "        # find the columns with missing values\n",
    "        columns_with_missing_values = df.columns[missing_values > 0]\n",
    "        print(\"Missing values per column:\")\n",
    "        print(missing_values[columns_with_missing_values])\n",
    "\n",
    "        # fix the missing values\n",
    "        # we can use df.replace(to_replace=' ', value=-1) to replace all missing values with -1\n",
    "        for column in columns_with_missing_values:\n",
    "\n",
    "            if option == 'drop':\n",
    "                df.dropna()\n",
    "            elif option == 'mean':\n",
    "                df[column].fillna(df[column].mean(), inplace=True)\n",
    "            elif option == 'median':\n",
    "                df[column].fillna(df[column].median(), inplace=True)\n",
    "            elif option == 'mode':\n",
    "                df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "            elif option == 'linear':\n",
    "                df[column].interpolate(method='linear', limit_direction='forward', inplace=True)\n",
    "            elif option == 'quadratic':\n",
    "                df[column].interpolate(method='quadratic', limit_direction='forward', inplace=True)\n",
    "            elif option == 'cubic':\n",
    "                df[column].interpolate(method='cubic', limit_direction='forward', inplace=True)\n",
    "            elif option == 'spline':\n",
    "                df[column].interpolate(method='spline', order=3, limit_direction='forward', inplace=True)\n",
    "        \n",
    "        print(\"Missing values per column are fixed\\n\")\n",
    "\n",
    "    else:\n",
    "        print(\"There are no missing values in the dataset.\\n\")\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_numeric_data(df, numeric_cols):\n",
    "    \"\"\"Normalize numeric columns in dataframe in place\"\"\"\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "    # print(numeric_cols)\n",
    "    scaler = MinMaxScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_encode(df, categorical_cols):\n",
    "    \"\"\"One hot encode categorical columns in dataframee\"\"\"\n",
    "    one_hot_encoded_data  = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "    categorical_cols = one_hot_encoded_data.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    label_encode(one_hot_encoded_data, categorical_cols)\n",
    "\n",
    "    return one_hot_encoded_data \n",
    "\n",
    "    \n",
    "def label_encode(df, categorical_cols):\n",
    "    \"\"\"Label encode categorical columns in dataframe in place\"\"\"\n",
    "\n",
    "    # print(categorical_cols)\n",
    "    \n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        label_encoders[col] = LabelEncoder()\n",
    "        df[col] = label_encoders[col].fit_transform(df[col])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_score_feature_df(X, y, k=15):\n",
    "    \"\"\"Get the top k features with the highest score\n",
    "        Return a new dataframe with the top k features\n",
    "    \"\"\"\n",
    "    # todo: need to implement this function\n",
    "    from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "    print (\"k:\",k)\n",
    "\n",
    "    selector = SelectKBest(mutual_info_classif, k=k)\n",
    "    \n",
    "    # Get the scores\n",
    "    selector.fit(X, y)\n",
    "    scores = selector.scores_\n",
    "\n",
    "    df_scores = pd.DataFrame(scores, columns=[\"Score\"], index=X.columns)\n",
    "    df_scores = df_scores.sort_values(by=\"Score\", ascending=False)\n",
    "\n",
    "    # keep the top k features of the df DataFrame\n",
    "    top_score_feature_X = X[df_scores.index[:k]]\n",
    "    # top_score_feature_df doesn't have the label column\n",
    "\n",
    "\n",
    "    # Plot the scores\n",
    "    # import matplotlib.pyplot as plt\n",
    "\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    # plt.barh(df_scores.index, df_scores[\"Score\"])\n",
    "    # plt.xlabel('Score')\n",
    "    # plt.ylabel('Feature')\n",
    "    # plt.title('Feature Importance')\n",
    "    # plt.show()\n",
    "    \n",
    "    return top_score_feature_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression:\n",
    "    \"\"\"Custom Logistic Regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate : float, default=0.01\n",
    "        Learning rate for gradient descent.\n",
    "\n",
    "    num_iterations : int, default=1000\n",
    "        Number of iterations for gradient descent.\n",
    "\n",
    "    verbose : bool, default=False\n",
    "        Print the progress of training if True.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    weights : 1d-array\n",
    "        Weights after fitting the model.\n",
    "\n",
    "    bias : float\n",
    "        Bias after fitting the model.\n",
    "\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    y_pred_class : 1d-array\n",
    "        Predictions using the trained model.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.verbose = verbose\n",
    "        self.weights = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y, early_stop_threshold = 0):\n",
    "        \"\"\"Fit the model using training data\"\"\"\n",
    "\n",
    "        \n",
    "\n",
    "        # Initialize the parameters\n",
    "        num_samples, num_features = X.shape\n",
    "        # print(\"num samples: \", num_samples)\n",
    "        # print(\"num features: \", num_features)\n",
    "        \n",
    "        # Initialize the weights randomly but reproducibly\n",
    "        np.random.seed(82)\n",
    "        self.weights = np.random.rand(num_features)\n",
    "\n",
    "        # Gradient descent\n",
    "        from tqdm import tqdm\n",
    "        for i in tqdm(range(self.num_iterations)):\n",
    "\n",
    "            linear_model = np.dot(X, self.weights)      # y hat = Xw\n",
    "            \n",
    "            # Predictions using sigmoid function\n",
    "            y_pred = self.sigmoid(linear_model)\n",
    "\n",
    "            # Calculate how many samples are misclassified \n",
    "            # y_pred_classified[y_pred >= 0.5] = 1\n",
    "            # y_pred_classified[y_pred < 0.5] = 0\n",
    "            y_pred_classified = [1 if i >= 0.5 else 0 for i in y_pred] # classifies the data into 0 or 1\n",
    "            num_correct_samples = sum([1 if y_pred_classified[i] == y[i] else 0 for i in range(len(y))])\n",
    "            \n",
    "            # print(\"num correct samples: \", num_correct_samples)\n",
    "            error = 1 - num_correct_samples / num_samples\n",
    "            \n",
    "            # Early terminate Gradient Descent if error in the training set becomes < early_stop_threshold\n",
    "            if error < early_stop_threshold and early_stop_threshold != 0:\n",
    "                print(f'Early stopping at Iteration: {i}')\n",
    "                break\n",
    "        \n",
    "\n",
    "            # Compute gradients\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (y_pred - y))\n",
    "            # print (dw)\n",
    "\n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw     # w = w - alpha * dw\n",
    "\n",
    "            if self.verbose and i % 100 == 0:\n",
    "                print(f'Iteration {i}, weights: {self.weights}, bias: {self.bias}')\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using the trained model\"\"\"\n",
    "        linear_model = np.dot(X, self.weights)      # y hat = Xw\n",
    "        y_pred = self.sigmoid(linear_model)\n",
    "        \n",
    "        y_pred_classified = [1 if i >= 0.5 else 0 for i in y_pred]\n",
    "        # make this into a numpy array\n",
    "        y_pred_classified = np.array(y_pred_classified)\n",
    "        \n",
    "        return y_pred_classified\n",
    "\n",
    "        \n",
    "    def performance(self, y_test, y_pred):\n",
    "        \"\"\"Compute performance\"\"\"\n",
    "\n",
    "        FP = 0\n",
    "        TP = 0\n",
    "        FN = 0\n",
    "        TN = 0\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 1 and y_test[i] == 0:\n",
    "                FP += 1\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 1 and y_test[i] == 1:\n",
    "                TP += 1\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 0 and y_test[i] == 1:\n",
    "                FN += 1\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 0 and y_test[i] == 0:\n",
    "                TN += 1\n",
    "\n",
    "        accuracy = (TP + TN) / len(y_pred)\n",
    "        sensitivity = TP / max( (TP + FN), 1e-10)\n",
    "        precision = TP / max( (TP + FP), 1e-10)\n",
    "        specificity = TN / max( (TN + FP), 1e-10)\n",
    "        false_discovery_rate = FP / max( (TP + FP), 1e-10)\n",
    "        f1 = 2 * precision * sensitivity / max((precision + sensitivity), 1e-10)\n",
    "\n",
    "        return accuracy, sensitivity,specificity, precision, false_discovery_rate, f1\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAdaBoostClassifier:\n",
    "    \"\"\"CustomAdaBoost Classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        the number of hypotheses in the ensemble\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    hypotheses : list\n",
    "        List of weak hypotheses.\n",
    "\n",
    "    alphas : list\n",
    "        List of weights of the weak hypotheses.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=10, learning_rate=0.01, num_iterations=1000, early_stop_threshold = 0, verbose=False):\n",
    "        self.k = k\n",
    "        self.hypotheses = None\n",
    "        self.alphas = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.verbose = verbose\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "  \n",
    "    \n",
    "    \n",
    "    def Resample(self, X, y, weights):\n",
    "        \"\"\"Resample the dataset using the weights\"\"\"\n",
    "\n",
    "        # Sample indices from the range of the length of y with replacement, using the weights\n",
    "        np.random.seed(82)\n",
    "        indices = np.random.choice(X.shape[0], X.shape[0], replace = True, p=weights)\n",
    "\n",
    "\n",
    "        print (indices)\n",
    "\n",
    "        # Create new dataset using the sampled indices\n",
    "\n",
    "        # print (X.shape)\n",
    "        # print (y.shape)\n",
    "        y_new = y[indices]\n",
    "        X_new = X.iloc[indices]\n",
    "\n",
    "        return X_new, y_new\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model using training data\"\"\"\n",
    "\n",
    "        # Initialize the weights\n",
    "        num_samples = X.shape[0]\n",
    "        weights = np.ones(num_samples) / num_samples\n",
    "\n",
    "        hypotheses = []\n",
    "        alphas = [] # weights of the hypotheses, Z in the pdf\n",
    "\n",
    "       \n",
    "        for _ in range(self.k):\n",
    "            # create a new dataset using the weights\n",
    "            X, y = self.Resample(X, y, weights)\n",
    "\n",
    "            # Train a weak hypothesis using the weights\n",
    "            hypothesis = CustomLogisticRegression(learning_rate=self.learning_rate, \n",
    "                                                    num_iterations=self.num_iterations, \n",
    "                                                    verbose=self.verbose)\n",
    "            \n",
    "            hypothesis.fit(X, y, early_stop_threshold=self.early_stop_threshold)\n",
    "\n",
    "            # Compute the error\n",
    "            y_pred = hypothesis.predict(X)\n",
    "            error = 0\n",
    "            for j in range(len(y_pred)):\n",
    "                if y_pred[j] != y[j]:\n",
    "                    error += weights[j]\n",
    "\n",
    "            # print (\"error:\", error) \n",
    "\n",
    "            if error > 0.5:\n",
    "                print(\"error is greater than 0.5\")\n",
    "                continue\n",
    "\n",
    "            # Update the weights of the samples. Give the correct predicted samples a lower weight\n",
    "            for i in range(len(weights)):\n",
    "                if y_pred[i] == y[i]:\n",
    "                    weights[i] *= error / (1 - error)\n",
    "            weights = weights / np.sum(weights)\n",
    "\n",
    "            hypotheses.append(hypothesis)\n",
    "            # Compute weights of the hypothesis\n",
    "            alpha = np.log((1 - error) / max(error, 1e-10))  # Avoid division by zero\n",
    "            alphas.append(alpha)\n",
    "\n",
    "\n",
    "        self.hypotheses = hypotheses\n",
    "\n",
    "        self.alphas = alphas/np.sum(alphas) # Normalize the weights of the hypotheses\n",
    "        print(\"alphas: \", self.alphas)\n",
    "\n",
    "\n",
    "\n",
    "    def predict_ada(self, X):\n",
    "        \"\"\"Predict using the trained model\"\"\"\n",
    "\n",
    "        num_samples = X.shape[0]\n",
    "        y_pred = np.zeros(num_samples)\n",
    "\n",
    "        print(\"hyp 1:\", type(self.hypotheses[0].predict(X)))\n",
    "        print(\"hyp 2:\", type(self.hypotheses[0].predict(X))[0])\n",
    "\n",
    "\n",
    "        for i in range(len(self.hypotheses)):\n",
    "            # weighted majority hypothesis     \n",
    "            y_pred += self.alphas[i] * self.hypotheses[i].predict(X)\n",
    "            # for j in range(num_samples):\n",
    "            #     y_pred[j] += self.alphas[i] * self.hypotheses[i].predict(X.iloc[j].values.reshape(1, -1))\n",
    "            \n",
    "\n",
    "        y_pred_classified = [1 if i >= 0.5 else 0 for i in y_pred] # classifies the data into 0 or 1\n",
    "    \n",
    "        return y_pred_classified\n",
    "        \n",
    "    \n",
    "\n",
    "    def performance(self, y_test, y_pred):\n",
    "        \"\"\"Compute performance\"\"\"\n",
    "\n",
    "        FP = 0\n",
    "        TP = 0\n",
    "        FN = 0\n",
    "        TN = 0\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 1 and y_test[i] == 0:\n",
    "                FP += 1\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 1 and y_test[i] == 1:\n",
    "                TP += 1\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 0 and y_test[i] == 1:\n",
    "                FN += 1\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 0 and y_test[i] == 0:\n",
    "                TN += 1\n",
    "\n",
    "        accuracy = (TP + TN) / len(y_pred)\n",
    "        sensitivity = TP / max( (TP + FN), 1e-10)\n",
    "        precision = TP / max( (TP + FP), 1e-10)\n",
    "        specificity = TN / max( (TN + FP), 1e-10)\n",
    "        false_discovery_rate = FP / max( (TP + FP), 1e-10)\n",
    "        f1 = 2 * precision * sensitivity / max((precision + sensitivity), 1e-10)\n",
    "\n",
    "        return accuracy, sensitivity,specificity, precision, false_discovery_rate, f1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def telco_customer_churn_preprocess(num_cols_keep = 10):\n",
    "    df = pd.read_csv('./data/Telco Customer Churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "    # print(df.head())\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    # errors='coerce' is a good approach to handle non-numeric values by replacing them with NaN.\n",
    "\n",
    "    df.replace(r'^\\s+$', np.nan, regex=True, inplace=True)\n",
    "    # drop customerID column because this will not help in prediction and cause trouble in one hot encoding\n",
    "    df.drop('customerID', axis=1, inplace=True)\n",
    "\n",
    "    label_col = df.columns[-1]\n",
    "\n",
    "\n",
    "    # Split into feature and label data\n",
    "    X = df.drop(columns=[label_col])\n",
    "    y = df[label_col]\n",
    "\n",
    "    numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    # Exclude columns with binary data\n",
    "    numeric_cols = [col for col in numeric_cols if len(X[col].unique()) > 2]\n",
    "    # print(numeric_cols)\n",
    "\n",
    "    categorical_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    # add the binary column back to the list of categorical columns\n",
    "    binary_cols =  [col for col in numeric_cols if len(X[col].unique()) == 2]\n",
    "    categorical_cols.extend(binary_cols)\n",
    "    X = one_hot_encode(X, categorical_cols)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=82)\n",
    "    # random_state is the seed used by the random number generator\n",
    "    # keeping the train and test data separate to avoid data leakage\n",
    "\n",
    "    handle_missing_values(X_train, option='mean')\n",
    "    handle_missing_values(X_test, option='mean')\n",
    "\n",
    "\n",
    "    normalize_numeric_data(X_train, numeric_cols)\n",
    "    normalize_numeric_data(X_test, numeric_cols)\n",
    "\n",
    "    y_train = LabelEncoder().fit_transform(y_train)\n",
    "    y_test = LabelEncoder().fit_transform(y_test)\n",
    "\n",
    "    X_train = get_top_score_feature_df(X_train, y_train, k=num_cols_keep)\n",
    "\n",
    "    # keep only the top k features in the test data from the train data\n",
    "    X_test = X_test[X_train.columns]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_telco_customer_churn_model(boosting_round = 5):\n",
    "    X_train, X_test, y_train, y_test = telco_customer_churn_preprocess(num_cols_keep=25)\n",
    "\n",
    "    model = CustomAdaBoostClassifier(\n",
    "        k=boosting_round,\n",
    "        learning_rate=0.01,\n",
    "        num_iterations=1000,\n",
    "        early_stop_threshold=0.1,\n",
    "        verbose=False,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_ada(X_test)\n",
    "\n",
    "    print(\"AdaBoost Classifier\")\n",
    "    accuracy, precision, recall, f1 = model.performance(y_test, y_pred)\n",
    "    print(\"Accuracy: \", accuracy * 100)\n",
    "    print(\"Precision: \", precision * 100)\n",
    "    print(\"Recall: \", recall * 100)\n",
    "    print(\"F1: \", f1 * 100)\n",
    "    \n",
    "    # save this report in a csv file\n",
    "    report = pd.DataFrame(columns=['Accuracy', 'Precision', 'Recall', 'F1'], index=['AdaBoost'])\n",
    "    report.loc['AdaBoost'] = [accuracy, precision, recall, f1]\n",
    "    report.to_csv('./telco_customer_report.csv')\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_regression(dataset_name):\n",
    "    if dataset_name == 'telco':\n",
    "        X_train, X_test, y_train, y_test = telco_customer_churn_preprocess()\n",
    "        \n",
    "\n",
    "    if dataset_name == 'credit':\n",
    "        X_train, X_test, y_train, y_test = credit_card_default_preprocessing()\n",
    "        \n",
    "\n",
    "    if dataset_name == 'adult':\n",
    "        X_train, X_test, y_train, y_test = adult_data_preprocessing()\n",
    "\n",
    "\n",
    "    model = CustomLogisticRegression(learning_rate=0.01, num_iterations=1000, verbose=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Logistic Regression Test\")\n",
    "    accuracy, sensitivity, specificity, precision, false_discovery_rate, f1 = model.performance(y_test, y_pred)\n",
    "    print(\"Accuracy: \", accuracy * 100)\n",
    "    print(\"Sensitivity: \", sensitivity * 100)\n",
    "    print(\"Specificity: \", specificity * 100)\n",
    "    print(\"Precision: \", precision * 100)\n",
    "    print(\"False Discovery Rate: \", false_discovery_rate * 100)\n",
    "    print(\"F1: \", f1 * 100)\n",
    "\n",
    "    # save this report in a csv file\n",
    "    report = pd.DataFrame(columns=['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'False Discovery Rate', 'F1'], index=['Logistic Regression Test'])\n",
    "    report.loc['Logistic Regression Test'] = [accuracy, sensitivity, specificity, precision, false_discovery_rate, f1]\n",
    "    report.transpose()\n",
    "    report.to_csv(f'./logistic_regression_{dataset_name}_report.csv')\n",
    "\n",
    "\n",
    "    y_pred = model.predict(X_train)\n",
    "\n",
    "    print(\"Logistic Regression Test\")\n",
    "    accuracy, sensitivity, specificity, precision, false_discovery_rate, f1 = model.performance(y_train, y_pred)\n",
    "    print(\"Accuracy: \", accuracy * 100)\n",
    "    print(\"Sensitivity: \", sensitivity * 100)\n",
    "    print(\"Specificity: \", specificity * 100)\n",
    "    print(\"Precision: \", precision * 100)\n",
    "    print(\"False Discovery Rate: \", false_discovery_rate * 100)\n",
    "    print(\"F1: \", f1 * 100)\n",
    "          \n",
    "    report = pd.DataFrame(columns=['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'False Discovery Rate', 'F1'], index=['Logistic Regression Train'])\n",
    "    report.loc['Logistic Regression Train'] = [accuracy, sensitivity, specificity, precision, false_discovery_rate, f1]\n",
    "    report.transpose()\n",
    "    report.to_csv(f'./logistic_regression_{dataset_name}_report.csv', mode='a')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_adaboost(dataset_name, boosting_round = 5):\n",
    "    if dataset_name == 'telco':\n",
    "        X_train, X_test, y_train, y_test = telco_customer_churn_preprocess()\n",
    "        \n",
    "\n",
    "    if dataset_name == 'credit':\n",
    "        X_train, X_test, y_train, y_test = credit_card_default_preprocessing()\n",
    "        \n",
    "\n",
    "    if dataset_name == 'adult':\n",
    "        X_train, X_test, y_train, y_test = adult_data_preprocessing()\n",
    "\n",
    "\n",
    "    model = CustomAdaBoostClassifier(\n",
    "        k=boosting_round,\n",
    "        learning_rate=0.01,\n",
    "        num_iterations=1000,\n",
    "        early_stop_threshold=0.1,\n",
    "        verbose=False,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_ada(X_test)\n",
    "\n",
    "    print(\"Adaboost Test\")\n",
    "    accuracy, sensitivity, specificity, precision, false_discovery_rate, f1 = model.performance(y_test, y_pred)\n",
    "    print(\"Accuracy: \", accuracy * 100)\n",
    "    print(\"Sensitivity: \", sensitivity * 100)\n",
    "    print(\"Specificity: \", specificity * 100)\n",
    "    print(\"Precision: \", precision * 100)\n",
    "    print(\"False Discovery Rate: \", false_discovery_rate * 100)\n",
    "    print(\"F1: \", f1 * 100)\n",
    "\n",
    "    # save this report in a csv file\n",
    "    report = pd.DataFrame(columns=['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'False Discovery Rate', 'F1'], index=['Adaboost Test'])\n",
    "    report.loc['Adaboost Test'] = [accuracy, sensitivity, specificity, precision, false_discovery_rate, f1]\n",
    "    report.transpose()\n",
    "    report.to_csv(f'./adaboost_{dataset_name}_report.csv')\n",
    "\n",
    "\n",
    "    y_pred = model.predict_ada(X_train)\n",
    "\n",
    "    print(\"Adaboost Train\")\n",
    "    accuracy, sensitivity, specificity, precision, false_discovery_rate, f1 = model.performance(y_train, y_pred)\n",
    "    print(\"Accuracy: \", accuracy * 100)\n",
    "    print(\"Sensitivity: \", sensitivity * 100)\n",
    "    print(\"Specificity: \", specificity * 100)\n",
    "    print(\"Precision: \", precision * 100)\n",
    "    print(\"False Discovery Rate: \", false_discovery_rate * 100)\n",
    "    print(\"F1: \", f1 * 100)\n",
    "          \n",
    "    report = pd.DataFrame(columns=['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'False Discovery Rate', 'F1'], index=['Adaboost Train'])\n",
    "    report.loc['Adaboost Train'] = [accuracy, sensitivity, specificity, precision, false_discovery_rate, f1]\n",
    "    report.transpose()\n",
    "    report.to_csv(f'./adaboost_{dataset_name}_report.csv', mode='a')\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit Card Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_default_preprocessing(num_cols_keep = 40):\n",
    "    df_credit = pd.read_csv('./data/Credit Card Fraud Detection/creditcard.csv')\n",
    "\n",
    "    df_credit.replace(r'^\\s+$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "    label_col_credit = df_credit.columns[-1]\n",
    "\n",
    "\n",
    "    # Split into feature and label data\n",
    "    X = df_credit.drop(columns=[label_col_credit])\n",
    "    y = df_credit[label_col_credit]\n",
    "\n",
    "    numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    # Exclude columns with binary data\n",
    "    numeric_cols = [col for col in numeric_cols if len(X[col].unique()) > 2]\n",
    "    # print(numeric_cols)\n",
    "\n",
    "    categorical_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    # add the binary column back to the list of categorical columns\n",
    "    binary_cols =  [col for col in numeric_cols if len(X[col].unique()) == 2]\n",
    "    categorical_cols.extend(binary_cols)\n",
    "    X = one_hot_encode(X, categorical_cols)\n",
    "\n",
    "    # take all the positive samples and 20000 of the negative samples\n",
    "    X = pd.concat([X[y==1], X[y==0].sample(20000)], axis=0)\n",
    "    y = pd.concat([y[y==1], y[y==0].sample(20000)], axis=0)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=82)\n",
    "    # random_state is the seed used by the random number generator\n",
    "    # keeping the train and test data separate to avoid data leakage\n",
    "\n",
    "    handle_missing_values(X_train, option='mean')\n",
    "    handle_missing_values(X_test, option='mean')\n",
    "\n",
    "\n",
    "    normalize_numeric_data(X_train, numeric_cols)\n",
    "    normalize_numeric_data(X_test, numeric_cols)\n",
    "\n",
    "    y_train = LabelEncoder().fit_transform(y_train)\n",
    "    y_test = LabelEncoder().fit_transform(y_test)\n",
    "\n",
    "    X_train = get_top_score_feature_df(X_train, y_train, k=num_cols_keep)\n",
    "\n",
    "    # keep only the top k features in the test data from the train data\n",
    "    X_test = X_test[X_train.columns]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_credit_card_model(boosting_round = 5):\n",
    "    X_train, X_test, y_train, y_test = credit_card_default_preprocessing(num_cols_keep=25)\n",
    "\n",
    "    model = CustomAdaBoostClassifier(\n",
    "        k=boosting_round,\n",
    "        learning_rate=0.01,\n",
    "        num_iterations=1000,\n",
    "        early_stop_threshold=0.1,\n",
    "        verbose=False,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_ada(X_test)\n",
    "\n",
    "    print(\"AdaBoost Classifier\")\n",
    "    accuracy, precision, recall, f1 = model.performance(y_test, y_pred)\n",
    "    print(\"Accuracy: \", accuracy * 100)\n",
    "    print(\"Precision: \", precision * 100)\n",
    "    print(\"Recall: \", recall * 100)\n",
    "    print(\"F1: \", f1 * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no missing values in the dataset.\n",
      "\n",
      "There are no missing values in the dataset.\n",
      "\n",
      "k: 25\n",
      "[ 4511 10478 10227 ... 15697  6516 12548]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:38<00:00, 26.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4506 10455 10205 ... 15684  6478 12547]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:38<00:00, 26.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4538 10422 10193 ... 15676  6489 12515]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:39<00:00, 25.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4549 10430 10198 ... 15682  6473 12513]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:38<00:00, 25.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphas:  [0.19193202 0.37425354 0.27279154 0.1610229 ]\n",
      "hyp 1: <class 'numpy.ndarray'>\n",
      "hyp 2: numpy.ndarray[0]\n",
      "AdaBoost Classifier\n",
      "Accuracy:  86.94803610636741\n",
      "Precision:  16.850393700787404\n",
      "Recall:  93.85964912280701\n",
      "F1:  28.571428571428577\n"
     ]
    }
   ],
   "source": [
    "# run_credit_card_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adult data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship' , \n",
    "                      'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "train_data = pd.read_csv('./data/adult/adult.data', names=col_names)\n",
    "test_data = pd.read_csv('./data/adult/adult.test', names=col_names, skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adult_income_preprocessing(num_cols_keep = 10):\n",
    "    label_col = train_data.columns[-1]\n",
    "\n",
    "    # Split into feature and label data\n",
    "    X_train = train_data.drop(columns=[label_col])\n",
    "    y_train = train_data[label_col]\n",
    "\n",
    "    X_test = test_data.drop(columns=[label_col])\n",
    "    y_test = test_data[label_col]\n",
    "    # keeping the train and test data separate to avoid data leakage\n",
    "\n",
    "    # modifying values in specific columns\n",
    "    y_train.replace({' >50K': 1, ' <=50K': 0}, inplace=True)\n",
    "    y_test.replace({' >50K': 1, ' <=50K.': 0}, inplace=True)\n",
    "\n",
    "    common_cols = list(set(X_train.columns) & set(X_test.columns))\n",
    "    X_train = X_train[common_cols]\n",
    "    X_test = X_test[common_cols]\n",
    "\n",
    "\n",
    "    numeric_cols_train = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "    # Exclude columns with binary data\n",
    "    numeric_cols_train = [col for col in numeric_cols_train if len(X[col].unique()) > 2]\n",
    "    # print(numeric_cols_train)\n",
    "\n",
    "    numeric_cols_test = X_test.select_dtypes(include=np.number).columns.tolist()\n",
    "    # Exclude columns with binary data\n",
    "    numeric_cols_test = [col for col in numeric_cols_test if len(X[col].unique()) > 2]\n",
    "\n",
    "    normalize_numeric_data(X_train, numeric_cols_train)\n",
    "    normalize_numeric_data(X_test, numeric_cols_train)\n",
    "\n",
    "    categorical_cols_train = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    # add the binary column back to the list of categorical columns\n",
    "    binary_cols_train =  [col for col in numeric_cols_train if len(X_train[col].unique()) == 2]\n",
    "    categorical_cols_train.extend(binary_cols_train)\n",
    "    X_train = one_hot_encode(X_train, categorical_cols_train)\n",
    "\n",
    "    categorical_cols_test = X_test.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    # add the binary column back to the list of categorical columns\n",
    "    binary_cols_test =  [col for col in numeric_cols_test if len(X_test[col].unique()) == 2]\n",
    "    categorical_cols_test.extend(binary_cols_test)\n",
    "    X_test = one_hot_encode(X_test, categorical_cols_test)\n",
    "\n",
    "\n",
    "    handle_missing_values(X_train, option='mean')\n",
    "    handle_missing_values(X_test, option='mean')\n",
    "\n",
    "\n",
    "\n",
    "    X_train = get_top_score_feature_df(X_train, y_train, k=num_cols_keep)\n",
    "    # keep only the top k features in the test data from the train data\n",
    "    X_test = X_test[X_train.columns]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_adult_model(boosting_round = 5):\n",
    "    X_train, X_test, y_train, y_test = adult_income_preprocessing(num_cols_keep = 15)\n",
    "\n",
    "    model = CustomAdaBoostClassifier(\n",
    "        k=boosting_round,\n",
    "        learning_rate=0.01,\n",
    "        num_iterations=1000,\n",
    "        early_stop_threshold=0.1,\n",
    "        verbose=False,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_ada(X_test)\n",
    "\n",
    "    print(\"AdaBoost Classifier\")\n",
    "    accuracy, precision, recall, f1 = model.performance(y_test, y_pred)\n",
    "    print(\"Accuracy: \", accuracy * 100)\n",
    "    print(\"Precision: \", precision * 100)\n",
    "    print(\"Recall: \", recall * 100)\n",
    "    print(\"F1: \", f1 * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no missing values in the dataset.\n",
      "\n",
      "There are no missing values in the dataset.\n",
      "\n",
      "k: 15\n",
      "[ 8961 20813 20314 ... 20291  7364 19085]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Pridesys\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:219\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:227\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:119\u001b[0m, in \u001b[0;36mpandas._libs.index._unpack_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Anup Personal\\BUET L4 T2\\CSE 472 ML Sessional\\OFFLINE 2\\solve.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m run_adult_model()\n",
      "\u001b[1;32md:\\Anup Personal\\BUET L4 T2\\CSE 472 ML Sessional\\OFFLINE 2\\solve.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m adult_income_preprocessing(num_cols_keep \u001b[39m=\u001b[39m \u001b[39m15\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m CustomAdaBoostClassifier(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     k\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_ada(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAdaBoost Classifier\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\Anup Personal\\BUET L4 T2\\CSE 472 ML Sessional\\OFFLINE 2\\solve.ipynb Cell 23\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# Train a weak hypothesis using the weights\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m hypothesis \u001b[39m=\u001b[39m CustomLogisticRegression(learning_rate\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m                                         num_iterations\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_iterations, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m                                         verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m hypothesis\u001b[39m.\u001b[39;49mfit(X, y, early_stop_threshold\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mearly_stop_threshold)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# Compute the error\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m y_pred \u001b[39m=\u001b[39m hypothesis\u001b[39m.\u001b[39mpredict(X)\n",
      "\u001b[1;32md:\\Anup Personal\\BUET L4 T2\\CSE 472 ML Sessional\\OFFLINE 2\\solve.ipynb Cell 23\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# Calculate how many samples are misclassified \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m y_pred_classified \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m y_pred] \u001b[39m# classifies the data into 0 or 1\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m num_correct_samples \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m y_pred_classified[i] \u001b[39m==\u001b[39m y[i] \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(y))])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m# print(\"num correct samples: \", num_correct_samples)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y100sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m error \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m num_correct_samples \u001b[39m/\u001b[39m num_samples\n",
      "File \u001b[1;32mc:\\Users\\Pridesys\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m   1039\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1040\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m   1042\u001b[0m \u001b[39m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[39m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\Pridesys\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1155\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1158\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\Pridesys\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "run_adult_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_adaboost(dataset_name, boosting_round = 5):\n",
    "    if dataset_name == 'telco':\n",
    "        run_telco_customer_churn_model(boosting_round)\n",
    "\n",
    "    if dataset_name == 'credit':\n",
    "        run_credit_card_model(boosting_round)\n",
    "\n",
    "    if dataset_name == 'adult':\n",
    "        run_adult_model(boosting_round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are missing values in the dataset.\n",
      "Missing values per column:\n",
      "TotalCharges    8\n",
      "dtype: int64\n",
      "Missing values per column are fixed\n",
      "\n",
      "There are missing values in the dataset.\n",
      "Missing values per column:\n",
      "TotalCharges    3\n",
      "dtype: int64\n",
      "Missing values per column are fixed\n",
      "\n",
      "k: 10\n",
      "[1550 3601 3515 ... 1257 2299 2085]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 84.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1548 3573 3494 ... 1256 2277 2077]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 84.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1539 3550 3479 ... 1247 2244 2053]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 94.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1535 3552 3481 ... 1247 2243 2051]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 86.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1532 3550 3477 ... 1247 2241 2048]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 85.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is greater than 0.5\n",
      "alphas:  [0.49689694 0.31254205 0.10108771 0.0894733 ]\n",
      "hyp 1: <class 'numpy.ndarray'>\n",
      "hyp 2: numpy.ndarray[0]\n",
      "Adaboost Test\n",
      "Accuracy:  73.88218594748048\n",
      "Sensitivity:  21.164021164021165\n",
      "Specificity:  93.21047526673134\n",
      "Precision:  53.333333333333336\n",
      "False Discovery Rate:  46.666666666666664\n",
      "F1:  30.303030303030305\n",
      "hyp 1: <class 'numpy.ndarray'>\n",
      "hyp 2: numpy.ndarray[0]\n",
      "Adaboost Train\n",
      "Accuracy:  74.86687965921193\n",
      "Sensitivity:  22.334004024144868\n",
      "Specificity:  93.77262853005068\n",
      "Precision:  56.34517766497462\n",
      "False Discovery Rate:  43.65482233502538\n",
      "F1:  31.98847262247838\n",
      "There are no missing values in the dataset.\n",
      "\n",
      "There are no missing values in the dataset.\n",
      "\n",
      "k: 40\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "k should be <= n_features = 30; got 40. Use k='all' to return all features.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Anup Personal\\BUET L4 T2\\CSE 472 ML Sessional\\OFFLINE 2\\solve.ipynb Cell 24\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     run_adaboost(\u001b[39m'\u001b[39m\u001b[39mtelco\u001b[39m\u001b[39m'\u001b[39m, i\u001b[39m*\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     run_adaboost(\u001b[39m'\u001b[39;49m\u001b[39mcredit\u001b[39;49m\u001b[39m'\u001b[39;49m, i\u001b[39m*\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# run_adaboost('adult', i*5)\u001b[39;00m\n",
      "\u001b[1;32md:\\Anup Personal\\BUET L4 T2\\CSE 472 ML Sessional\\OFFLINE 2\\solve.ipynb Cell 24\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m telco_customer_churn_preprocess()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m dataset_name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcredit\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m credit_card_default_preprocessing()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m dataset_name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39madult\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m adult_data_preprocessing()\n",
      "\u001b[1;32md:\\Anup Personal\\BUET L4 T2\\CSE 472 ML Sessional\\OFFLINE 2\\solve.ipynb Cell 24\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m y_train \u001b[39m=\u001b[39m LabelEncoder()\u001b[39m.\u001b[39mfit_transform(y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m y_test \u001b[39m=\u001b[39m LabelEncoder()\u001b[39m.\u001b[39mfit_transform(y_test)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m X_train \u001b[39m=\u001b[39m get_top_score_feature_df(X_train, y_train, k\u001b[39m=\u001b[39;49mnum_cols_keep)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# keep only the top k features in the test data from the train data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m X_test \u001b[39m=\u001b[39m X_test[X_train\u001b[39m.\u001b[39mcolumns]\n",
      "\u001b[1;32md:\\Anup Personal\\BUET L4 T2\\CSE 472 ML Sessional\\OFFLINE 2\\solve.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m selector \u001b[39m=\u001b[39m SelectKBest(mutual_info_classif, k\u001b[39m=\u001b[39mk)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Get the scores\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m selector\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m scores \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39mscores_\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Anup%20Personal/BUET%20L4%20T2/CSE%20472%20ML%20Sessional/OFFLINE%202/solve.ipynb#Y105sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m df_scores \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(scores, columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m\"\u001b[39m], index\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mcolumns)\n",
      "File \u001b[1;32mc:\\Users\\Pridesys\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pridesys\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:502\u001b[0m, in \u001b[0;36m_BaseFilter.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run score function on (X, y) and get the appropriate features.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \n\u001b[0;32m    484\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[39m    Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    498\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m    499\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m], multi_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    500\u001b[0m )\n\u001b[1;32m--> 502\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_params(X, y)\n\u001b[0;32m    503\u001b[0m score_func_ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore_func(X, y)\n\u001b[0;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(score_func_ret, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n",
      "File \u001b[1;32mc:\\Users\\Pridesys\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:703\u001b[0m, in \u001b[0;36mSelectKBest._check_params\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_params\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[0;32m    702\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk \u001b[39m>\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[1;32m--> 703\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    704\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mk should be <= n_features = \u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    705\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk\u001b[39m}\u001b[39;00m\u001b[39m. Use k=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to return all features.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    706\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: k should be <= n_features = 30; got 40. Use k='all' to return all features."
     ]
    }
   ],
   "source": [
    "for i in range(1, 2):\n",
    "    run_adaboost('telco', i*5)\n",
    "    run_adaboost('credit', i*5)\n",
    "    # run_adaboost('adult', i*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are missing values in the dataset.\n",
      "Missing values per column:\n",
      "TotalCharges    8\n",
      "dtype: int64\n",
      "Missing values per column are fixed\n",
      "\n",
      "There are missing values in the dataset.\n",
      "Missing values per column:\n",
      "TotalCharges    3\n",
      "dtype: int64\n",
      "Missing values per column are fixed\n",
      "\n",
      "k: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 89.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test\n",
      "Accuracy:  74.73385379701917\n",
      "Sensitivity:  21.164021164021165\n",
      "Specificity:  94.37439379243453\n",
      "Precision:  57.971014492753625\n",
      "False Discovery Rate:  42.028985507246375\n",
      "F1:  31.007751937984494\n",
      "Logistic Regression Test\n",
      "Accuracy:  75.64785232516861\n",
      "Sensitivity:  22.065727699530516\n",
      "Specificity:  94.93120926864592\n",
      "Precision:  61.038961038961034\n",
      "False Discovery Rate:  38.961038961038966\n",
      "F1:  32.41379310344828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_logistic_regression('telco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
